{
  "id": 5627,
  "map": {
    "variableId": 180076
  },
  "data": {
    "availableEntities": [
      "Theseus",
      "SNARC",
      "Self Organizing System",
      "Perceptron Mark I",
      "Pandemonium (morse)",
      "Samuel Neural Checkers",
      "ADALINE",
      "LMS",
      "MADALINE I",
      "STeLLA",
      "MENACE",
      "Samuel Neural Checkers II",
      "GLEE",
      "BOXES",
      "Bootstrap Adaptation",
      "Naive Bayes",
      "Cognitron",
      "TD(0)",
      "Neocognitron",
      "Kohonen network",
      "Hopfield network",
      "ASE+ACE",
      "Back-propagation",
      "NetTalk",
      "MADALINE II",
      "Adaptive Broom Balancer",
      "Q-learning",
      "Time-delay neural networks",
      "ALVINN",
      "Zip CNN",
      "MADALINE III",
      "DIABETES",
      "TD-Gammon",
      "Fuzzy NN",
      "IBM-5",
      "GroupLens",
      "Random Decision Forests",
      "Support Vector Machines",
      "System 11",
      "HMM Word Alignment",
      "BRNN",
      "LSTM",
      "RNN for speech",
      "LeNet-5",
      "IBM Model 4",
      "Peephole LSTM",
      "Immediate trihead",
      "Decision tree (classification)",
      "Thumbs Up?",
      "LDA",
      "NPLM",
      "Phrase-based translation",
      "CNN Best Practices",
      "Sandstorm (DARPA Grand Challenge I)",
      "LIRA",
      "SACHS",
      "Hiero",
      "BiLSTM for Speech",
      "Stanley /DARPA Grand Challenge 2)",
      "FAST",
      "DrLIM",
      "CTC-Trained LSTM",
      "DImensionality Reduction",
      "Deep Belief Nets",
      "λ-WASP",
      "Restricted Bolzmann machines",
      "BLSTM",
      "Denoising Autoencoders",
      "Stacked Semisuperviser Autoencoders",
      "Boss (DARPA Urban Challenge)",
      "BigChaos 2008",
      "Semantic Hashing",
      "Deep Boltzmann Machines",
      "GPU DBNs",
      "BellKor 2008",
      "RL mapping instructions 2",
      "RL mapping instructions",
      "BellKor 2007",
      "6-layer MLP (MNIST)",
      "Feedforward NN",
      "Word Representations",
      "Deconvolutional Network",
      "ReLU (NORB)",
      "ReLU (LFW)",
      "RNN 500/10 + RT09 LM (NIST RT05)",
      "KN5 LM + RNN 400/10 (WSJ)",
      "Domain Adaptation",
      "NLP from scratch",
      "MCDNN (MNIST)",
      "Dropout (MNIST)",
      "Dropout (TIMIT)",
      "Dropout (CIFAR)",
      "Dropout (ImageNet)",
      "MV-RNN",
      "AlexNet",
      "DQN",
      "Maxout Networks ",
      "PreTrans-3L-250H",
      "Mitosis",
      "Word2Vec (large)",
      "Word2Vec (small)",
      "R-CNN (T-net)",
      "Visualizing CNNs",
      "TransE",
      "DBLSTM",
      "Network in Network",
      "Image generation",
      "GloVe (6B)",
      "GloVe (32B)",
      "DBNs",
      "HyperNEAT",
      "GRUs",
      "GANs",
      "SPPNet",
      "Multiresolution CNN",
      "SmooCT",
      "RNNsearch-50*",
      "VGG16",
      "VGG19",
      "Seq2Seq",
      "LRCN",
      "NTM",
      "ADAM (CIFAR-10)",
      "DeepLab",
      "MSRA (C, PReLU)",
      "DQN-2015",
      "Constituency-Tree LSTM",
      "Fast R-CNN",
      "DSN",
      "Faster R-CNN",
      "GoogLeNet / InceptionV1",
      "YOLO",
      "BatchNorm",
      "BPE",
      "AlphaGo Fan",
      "Inception v3",
      "DeepSpeech2",
      "ResNet-152 (ImageNet)",
      "ResNet-110 (CIFAR-10)",
      "BPL",
      "Advantage Learning",
      "AlphaGo Lee",
      "A3C FF hs",
      "Inceptionv4",
      "Inception-ResNet-V2",
      "SqueezeNet",
      "DMN",
      "R-FCN",
      "Part-of-sentence tagging model",
      "Named Entity Recognition model",
      "DenseNet-264",
      "Stacked hourglass network",
      "MS-CNN",
      "Wide Residual Network",
      "GNMT",
      "Xception",
      "NASv3 (CIFAR-10)",
      "ResNeXt-50",
      "PolyNet",
      "RefineNet",
      "PointNet",
      "YOLOv2",
      "Libratus",
      "AlphaGo Master",
      "DeepStack",
      "MoE",
      "Mask R-CNN",
      "MobileNet",
      "PointNet++",
      "Transformer",
      "HRA",
      "DeepLabV3",
      "NoisyNet-Dueling",
      "ShuffleNet v1",
      "NASNet-A",
      "JFT",
      "RetinaNet-R50",
      "RetinaNet-R50 2",
      "OpenAI TI7 DOTA 1v1",
      "NeuMF (Pinterest)",
      "SENet (ImageNet)",
      "AlphaGo Zero",
      "CapsNet (MNIST)",
      "CapsNet (MultiMNIST)",
      "PNASNet-5",
      "PNAS-net",
      "AlphaZero",
      "Refined Part Pooling",
      "ULM-FiT",
      "ELMo",
      "IMPALA",
      "AmoebaNet-A",
      "AmoebaNet-A 2",
      "DeepLabV3+",
      "Chinese - English translation",
      "Rotation",
      "YOLOv3",
      "ResNeXt-101 32x48d",
      "GPT",
      "MobileNetV2",
      "ShuffleNet v2",
      "Population-based DRL",
      "ESRGAN",
      "BigGAN-deep 512x512",
      "BERT-Large",
      "MetaMimic",
      "GPipe (Amoeba)",
      "GPipe (Transformer)",
      "Transformer ELMo",
      "Decoupled weight decay regularization",
      "MT-DNN",
      "Hanabi 4 player",
      "GPT-2",
      "ProxylessNAS",
      "Cross-lingual alignment",
      "ResNet-50 Billion-scale",
      "ResNeXt-101 Billion-scale",
      "CPC v2",
      "EfficientNet-L2",
      "MnasNet-A1 + SSDLite",
      "MnasNet-A3",
      "Grover-Mega",
      "DLRM-2020",
      "FTW",
      "XLM",
      "XLNet",
      "AMDIM",
      "FixRes ResNeXt-101 WSL",
      "RoBERTa",
      "BigBiGAN",
      "ObjectNet",
      "Hide and Seek",
      "Megatron-LM",
      "Megatron-BERT",
      "ALBERT",
      "AlphaX-1",
      "DistilBERT",
      "Rubik's cube",
      "T5-3B",
      "T5-11B",
      "BART-large",
      "AlphaStar",
      "Noisy Student (L2)",
      "MoCo",
      "MuZero",
      "StarGAN v2",
      "OpenAI Five",
      "OpenAI Five Rerun",
      "Big Transfer (BiT-L)",
      "AlphaFold",
      "Meena",
      "Theseus 6/768",
      "Perceiver IO",
      "ALBERT-xxlarge",
      "Turing NLG",
      "SimCLR",
      "ProGen",
      "ELECTRA",
      "MetNet",
      "Agent57",
      "MobileBERT",
      "CURL",
      "Go-explore",
      "GPT-3 175B",
      "Once for All",
      "SqueezeBERT",
      "iGPT-L",
      "iGPT-XL",
      "GShard (600B)",
      "GShard (dense)",
      "DLRM-2021",
      "Hopfield Networks (2020)",
      "EfficientDet",
      "ERNIE-GEN (large)",
      "ViT-H/14",
      "wave2vec 2.0 LARGE",
      "ViT-Base/32",
      "ViT-Huge/14",
      "SimCLRv2",
      "KEPLER",
      "AlphaFold2",
      "CPM-Large",
      "VQGAN + CLIP",
      "AraGPT2-Mega",
      "CLIP (ViT L/14@336px)",
      "DALL-E",
      "CLIP (ResNet-50)",
      "BigSSL",
      "Switch",
      "FLAN",
      "Rational DQN Average",
      "Meta Pseudo Labels",
      "M6-10B",
      "M6-100B",
      "Wu Dao - Wen Yuan",
      "Wu Dao - Wen Lan",
      "Wu Dao - Wen Hui",
      "Wu Dao - Wen Su",
      "M6-T",
      "GPT-Neo",
      "DLRM-12T",
      "PanGu-α",
      "GPT-J-6B",
      "ProtT5-XXL",
      "HyperClova",
      "CogView",
      "Transformer local-attention (NesT-B)",
      "Wu Dao 2.0",
      "ViT-G/14",
      "DeBERTa",
      "ALIGN",
      "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
      "ERNIE 3.0",
      "Codex",
      "HuBERT",
      "GOAT",
      "SEER",
      "Jurassic-1-Jumbo",
      "XLMR-XXL",
      "MEB",
      "NEO (DL:RM-2022)",
      "M6-10T",
      "Megatron-Turing NLG 530B",
      "Yuan 1.0",
      "T0-XXL",
      "Cloob",
      "EfficientZero",
      "Japanese dialog transformers",
      "NÜWA",
      "Player of Games",
      "Gopher",
      "GLIDE",
      "PCL-BAIDU Wenxin (ERNIE 3.0 Titan)",
      "data2vec (vision)",
      "data2vec (speech)",
      "data2vec (language)",
      "Primer",
      "InstructGPT",
      "AlphaCode",
      "RETRO-7B",
      "GPT-NeoX-20B",
      "LaMDA",
      "DeepNet",
      "Statement Curriculum Learning",
      "BaGuaLu",
      "Chinchilla",
      "PaLM (540B)",
      "DALL·E 2",
      "Sparse all-MLP",
      "Flamingo",
      "OPT-175B",
      "Jurassic-X",
      "UL2",
      "Gato",
      "Imagen",
      "MetaLM",
      "Parti",
      "Minerva (540B)",
      "NLLB"
    ]
  },
  "note": "The estimates have some uncertainty but are expected to be correct within a factor of ~2.  If the authors had affiliations in both Academia and Industry, the sector was labeled Industry because Industry-controlled computation is preferred in practice.",
  "slug": "ai-training-computation-by-sector",
  "type": "ScatterPlot",
  "title": "Estimated computation used in large AI training runs, by sector providing computation",
  "xAxis": {
    "label": "Publication date"
  },
  "yAxis": {
    "scaleType": "log",
    "canChangeScaleType": true
  },
  "version": 108,
  "subtitle": "Selection of notable AI systems that used a large amount of computation in training. Computation is measured in petaFLOPs, which is 10¹⁵ floating-point operations. Sector is based on affiliation of the research paper authors.",
  "colorScale": {
    "baseColorScheme": "owid-distinct",
    "binningStrategy": "manual",
    "legendDescription": "Sector",
    "customCategoryColors": {
      "Academia": "#3c4e66",
      "Industry": "#b13507",
      "Collaboration": "#a2559c",
      "Uncategorized": "#969696",
      "Research Collective": "#00847d",
      "Collaboration, Academia-leaning": "#4393c3",
      "Collaboration, Industry-leaning": "#d6604d"
    },
    "customCategoryLabels": {
      "games": "Games",
      "speech": "Speech",
      "vision": "Vision",
      "No data": "Other",
      "driving": "Driving",
      "general": "General",
      "language": "Language"
    },
    "customNumericColorsActive": true
  },
  "dimensions": [
    {
      "display": {
        "name": "petaFLOPs",
        "unit": "petaFLOPs",
        "shortUnit": "",
        "includeInTable": true
      },
      "property": "y",
      "variableId": 312259
    },
    {
      "property": "color",
      "variableId": 412581
    }
  ],
  "entityType": "system",
  "isPublished": true,
  "baseColorScheme": "owid-distinct",
  "zoomToSelection": true,
  "entityTypePlural": "systems",
  "hideTitleAnnotation": true,
  "hideConnectedScatterLines": false
}